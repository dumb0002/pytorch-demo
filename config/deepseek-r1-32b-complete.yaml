apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: deepseek-r1-32b
spec:
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: sza
  rules:
    - backendRefs:
      - group: inference.networking.x-k8s.io
        kind: InferencePool
        name: deepseek-r1-32b-epp
        port: 8000
      filters:
      - type: URLRewrite
        urlRewrite:
          path:
            replacePrefixMatch: /
            type: ReplacePrefixMatch
      matches:
      - path:
          type: PathPrefix
          value: /deepseek32
      timeouts:
        request: 300s
---
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  name: deepseek-r1-32b-epp
spec:
  host: deepseek-r1-32b-epp.pytorch-demo.svc.cluster.local
  trafficPolicy:
    tls:
      insecureSkipVerify: true
      mode: SIMPLE
---
apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferencePool
metadata:
  name: deepseek-r1-32b-epp
  annotations:
    activator.llm-d.ai/scale-down-delay: "600s"
    activator.llm-d.ai/scale-from-zero-grace-period: "300s"
    activator.llm-d.ai/target-apiversion: apps/v1
    activator.llm-d.ai/target-kind: Deployment
    activator.llm-d.ai/target-name: deepseek-r1-32b
spec:
  extensionRef:
    failureMode: FailClose
    group: ""
    kind: Service
    name: deepseek-r1-32b-epp
    portNumber: 9002
  selector:
    llm-d.ai/model: "deepseek-r1-32b"
    llm-d.ai/inferenceServing: "true"
  targetPortNumber: 8000
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deepseek-r1-32b-epp
  labels:
    app: deepseek-r1-32b-epp
spec:
  replicas: 1
  selector:
    matchLabels:
      inferencepool: deepseek-r1-32b-epp
  template:
    metadata:
      labels:
        inferencepool: deepseek-r1-32b-epp
    spec:
      containers:
      - name: epp
        image: ghcr.io/llm-d/llm-d-inference-scheduler:v0.3.2
        args:
        - --pool-name
        - deepseek-r1-32b-epp
        - --pool-namespace
        - pytorch-demo
        - --pool-group
        - inference.networking.x-k8s.io
        - --zap-encoder
        - json
        - --config-file
        - /config/default-plugins.yaml
        - --v
        - "4"
        env:
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
        livenessProbe:
          failureThreshold: 3
          grpc:
            port: 9003
            service: inference-extension
          initialDelaySeconds: 1
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        ports:
        - containerPort: 9002
          name: grpc
          protocol: TCP
        - containerPort: 9003
          name: grpc-health
          protocol: TCP
        - containerPort: 9090
          name: metrics
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          grpc:
            port: 9003
            service: inference-extension
          periodSeconds: 2
          successThreshold: 1
          timeoutSeconds: 1
        volumeMounts:
        - mountPath: /config
          name: plugins-config-volume
      serviceAccountName: epp
      volumes:
      - configMap:
          defaultMode: 420
          name: epp
        name: plugins-config-volume
---
apiVersion: v1
kind: Service
metadata:
  name: deepseek-r1-32b-epp
spec:
  selector:
    inferencepool: deepseek-r1-32b-epp
  ports:
  - name: grpc-ext-proc
    port: 9002
    protocol: TCP
    targetPort: 9002
  - name: http-metrics
    port: 9090
    protocol: TCP
    targetPort: 9090
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deepseek-r1-32b
spec:
  replicas: 0
  selector:
    matchLabels:
      llm-d.ai/model: "deepseek-r1-32b"
      llm-d.ai/inferenceServing: "true"
  template:
    metadata:
      labels:
        llm-d.ai/model: "deepseek-r1-32b"
        llm-d.ai/inferenceServing: "true"
    spec:
      containers:
      - args:
        - --model
        - deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
        - --port
        - "8000"
        image: ghcr.io/llm-d/llm-d-inference-sim:v0.5.1
        imagePullPolicy: IfNotPresent
        name: vllm-sim
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
        ports:
        - containerPort: 8000
          name: http
          protocol: TCP